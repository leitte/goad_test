{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "goad.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "pretty-rachel"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.io\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import precision_recall_fscore_support as prf\n",
        "\n",
        "import torchvision.datasets as dset\n",
        "import os\n",
        "\n",
        "from types import SimpleNamespace"
      ],
      "id": "pretty-rachel",
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "difficult-colorado"
      },
      "source": [
        "class Data_Loader:\n",
        "\n",
        "    def __init__(self, n_trains=None):\n",
        "        self.n_train = n_trains\n",
        "        self.urls = [\n",
        "        \"http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz\",\n",
        "        \"http://kdd.ics.uci.edu/databases/kddcup99/kddcup.names\"\n",
        "        ]\n",
        "\n",
        "    def norm_kdd_data(self, train_real, val_real, val_fake, cont_indices):\n",
        "        symb_indices = np.delete(np.arange(train_real.shape[1]), cont_indices)\n",
        "        mus = train_real[:, cont_indices].mean(0)\n",
        "        sds = train_real[:, cont_indices].std(0)\n",
        "        sds[sds == 0] = 1\n",
        "\n",
        "        def get_norm(xs, mu, sd):\n",
        "            bin_cols = xs[:, symb_indices]\n",
        "            cont_cols = xs[:, cont_indices]\n",
        "            cont_cols = np.array([(x - mu) / sd for x in cont_cols])\n",
        "            return np.concatenate([bin_cols, cont_cols], 1)\n",
        "\n",
        "        train_real = get_norm(train_real, mus, sds)\n",
        "        val_real = get_norm(val_real, mus, sds)\n",
        "        val_fake = get_norm(val_fake, mus, sds)\n",
        "        return train_real, val_real, val_fake\n",
        "\n",
        "\n",
        "    def norm_data(self, train_real, val_real, val_fake):\n",
        "        mus = train_real.mean(0)\n",
        "        sds = train_real.std(0)\n",
        "        sds[sds == 0] = 1\n",
        "\n",
        "        def get_norm(xs, mu, sd):\n",
        "            return np.array([(x - mu) / sd for x in xs])\n",
        "\n",
        "        train_real = get_norm(train_real, mus, sds)\n",
        "        val_real = get_norm(val_real, mus, sds)\n",
        "        val_fake = get_norm(val_fake, mus, sds)\n",
        "        return train_real, val_real, val_fake\n",
        "\n",
        "    def norm(self, data, mu=1):\n",
        "        return 2 * (data / 255.) - mu\n",
        "\n",
        "    def get_dataset(self, dataset_name, c_percent=None, true_label=1):\n",
        "        if dataset_name == 'cifar10':\n",
        "            return self.load_data_CIFAR10(true_label)\n",
        "        if dataset_name == 'kdd':\n",
        "            return self.KDD99_train_valid_data()\n",
        "        if dataset_name == 'kddrev':\n",
        "            return self.KDD99Rev_train_valid_data()\n",
        "        if dataset_name == 'thyroid':\n",
        "            return self.Thyroid_train_valid_data()\n",
        "        if dataset_name == 'arrhythmia':\n",
        "            return self.Arrhythmia_train_valid_data()\n",
        "        if dataset_name == 'ckdd':\n",
        "            return self.contaminatedKDD99_train_valid_data(c_percent)\n",
        "\n",
        "\n",
        "    def load_data_CIFAR10(self, true_label):\n",
        "        root = './data'\n",
        "        if not os.path.exists(root):\n",
        "            os.mkdir(root)\n",
        "\n",
        "        trainset = dset.CIFAR10(root, train=True, download=True)\n",
        "        train_data = np.array(trainset.data)\n",
        "        train_labels = np.array(trainset.targets)\n",
        "\n",
        "        testset = dset.CIFAR10(root, train=False, download=True)\n",
        "        test_data = np.array(testset.data)\n",
        "        test_labels = np.array(testset.targets)\n",
        "\n",
        "        train_data = train_data[np.where(train_labels == true_label)]\n",
        "        x_train = self.norm(np.asarray(train_data, dtype='float32'))\n",
        "        x_test = self.norm(np.asarray(test_data, dtype='float32'))\n",
        "        return x_train, x_test, test_labels\n",
        "\n",
        "\n",
        "    def Thyroid_train_valid_data(self):\n",
        "        data = scipy.io.loadmat(\"./thyroid.mat\")\n",
        "        samples = data['X']  # 3772\n",
        "        labels = ((data['y']).astype(np.int32)).reshape(-1)\n",
        "\n",
        "        norm_samples = samples[labels == 0]  # 3679 norm\n",
        "        anom_samples = samples[labels == 1]  # 93 anom\n",
        "\n",
        "        n_train = len(norm_samples) // 2\n",
        "        x_train = norm_samples[:n_train]  # 1839 train\n",
        "\n",
        "        val_real = norm_samples[n_train:]\n",
        "        val_fake = anom_samples\n",
        "        return self.norm_data(x_train, val_real, val_fake)\n",
        "\n",
        "\n",
        "    def Arrhythmia_train_valid_data(self):\n",
        "        data = scipy.io.loadmat(\"./arrhythmia.mat\")\n",
        "        samples = data['X']  # 518\n",
        "        labels = ((data['y']).astype(np.int32)).reshape(-1)\n",
        "\n",
        "        norm_samples = samples[labels == 0]  # 452 norm\n",
        "        anom_samples = samples[labels == 1]  # 66 anom\n",
        "\n",
        "        n_train = len(norm_samples) // 2\n",
        "        x_train = norm_samples[:n_train]  # 226 train\n",
        "\n",
        "        val_real = norm_samples[n_train:]\n",
        "        val_fake = anom_samples\n",
        "        return self.norm_data(x_train, val_real, val_fake)\n",
        "\n",
        "\n",
        "    def KDD99_preprocessing(self):\n",
        "        df_colnames = pd.read_csv(self.urls[1], skiprows=1, sep=':', names=['f_names', 'f_types'])\n",
        "        df_colnames.loc[df_colnames.shape[0]] = ['status', ' symbolic.']\n",
        "        df = pd.read_csv(self.urls[0], header=None, names=df_colnames['f_names'].values)\n",
        "        df_symbolic = df_colnames[df_colnames['f_types'].str.contains('symbolic.')]\n",
        "        df_continuous = df_colnames[df_colnames['f_types'].str.contains('continuous.')]\n",
        "        samples = pd.get_dummies(df.iloc[:, :-1], columns=df_symbolic['f_names'][:-1])\n",
        "\n",
        "        smp_keys = samples.keys()\n",
        "        cont_indices = []\n",
        "        for cont in df_continuous['f_names']:\n",
        "            cont_indices.append(smp_keys.get_loc(cont))\n",
        "\n",
        "        labels = np.where(df['status'] == 'normal.', 1, 0)\n",
        "        return np.array(samples), np.array(labels), cont_indices\n",
        "\n",
        "\n",
        "    def KDD99_train_valid_data(self):\n",
        "        samples, labels, cont_indices = self.KDD99_preprocessing()\n",
        "        anom_samples = samples[labels == 1]  # norm: 97278\n",
        "\n",
        "        norm_samples = samples[labels == 0]  # attack: 396743\n",
        "\n",
        "        n_norm = norm_samples.shape[0]\n",
        "        ranidx = np.random.permutation(n_norm)\n",
        "        n_train = n_norm // 2\n",
        "\n",
        "        x_train = norm_samples[ranidx[:n_train]]\n",
        "        norm_test = norm_samples[ranidx[n_train:]]\n",
        "\n",
        "        val_real = norm_test\n",
        "        val_fake = anom_samples\n",
        "        return self.norm_kdd_data(x_train, val_real, val_fake, cont_indices)\n",
        "\n",
        "\n",
        "    def KDD99Rev_train_valid_data(self):\n",
        "        samples, labels, cont_indices = self.KDD99_preprocessing()\n",
        "\n",
        "        norm_samples = samples[labels == 1]  # norm: 97278\n",
        "\n",
        "        # Randomly draw samples labeled as 'attack'\n",
        "        # so that the ratio btw norm:attack will be 4:1\n",
        "        # len(anom) = 24,319\n",
        "        anom_samples = samples[labels == 0]  # attack: 396743\n",
        "\n",
        "        rp = np.random.permutation(len(anom_samples))\n",
        "        rp_cut = rp[:24319]\n",
        "        anom_samples = anom_samples[rp_cut]  # attack:24319\n",
        "\n",
        "        n_norm = norm_samples.shape[0]\n",
        "        ranidx = np.random.permutation(n_norm)\n",
        "        n_train = n_norm // 2\n",
        "\n",
        "        x_train = norm_samples[ranidx[:n_train]]\n",
        "        norm_test = norm_samples[ranidx[n_train:]]\n",
        "\n",
        "        val_real = norm_test\n",
        "        val_fake = anom_samples\n",
        "        return self.norm_kdd_data(x_train, val_real, val_fake, cont_indices)\n",
        "\n",
        "\n",
        "    def contaminatedKDD99_train_valid_data(self, c_percent):\n",
        "        samples, labels, cont_indices = self.KDD99_preprocessing()\n",
        "\n",
        "        ranidx = np.random.permutation(len(samples))\n",
        "        n_test = len(samples)//2\n",
        "        x_test = samples[ranidx[:n_test]]\n",
        "        y_test = labels[ranidx[:n_test]]\n",
        "\n",
        "        x_train = samples[ranidx[n_test:]]\n",
        "        y_train = labels[ranidx[n_test:]]\n",
        "\n",
        "        norm_samples = x_train[y_train == 0]  # attack: 396743\n",
        "        anom_samples = x_train[y_train == 1]  # norm: 97278\n",
        "        n_contaminated = int((c_percent/100)*len(anom_samples))\n",
        "\n",
        "        rpc = np.random.permutation(n_contaminated)\n",
        "        x_train = np.concatenate([norm_samples, anom_samples[rpc]])\n",
        "\n",
        "        val_real = x_test[y_test == 0]\n",
        "        val_fake = x_test[y_test == 1]\n",
        "        return self.norm_kdd_data(x_train, val_real, val_fake, cont_indices)"
      ],
      "id": "difficult-colorado",
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "growing-incidence"
      },
      "source": [
        "def tc_loss(zs, m):\n",
        "    means = zs.mean(0).unsqueeze(0)\n",
        "    res = ((zs.unsqueeze(2) - means.unsqueeze(1)) ** 2).sum(-1)\n",
        "    pos = torch.diagonal(res, dim1=1, dim2=2)\n",
        "    offset = torch.diagflat(torch.ones(zs.size(1))).unsqueeze(0).cuda() * 1e6\n",
        "    neg = (res + offset).min(-1)[0]\n",
        "    loss = torch.clamp(pos + m - neg, min=0).mean()\n",
        "    return loss\n",
        "\n",
        "def f_score(scores, labels, ratio):\n",
        "    thresh = np.percentile(scores, ratio)\n",
        "    y_pred = (scores >= thresh).astype(int)\n",
        "    y_true = labels.astype(int)\n",
        "    precision, recall, f_score, support = prf(y_true, y_pred, average='binary')\n",
        "    return f_score\n",
        "\n",
        "\n",
        "class TransClassifierTabular():\n",
        "    def __init__(self, args):\n",
        "        self.ds = args.dataset\n",
        "        self.m = args.m\n",
        "        self.lmbda = args.lmbda\n",
        "        self.batch_size = args.batch_size\n",
        "        self.ndf = args.ndf\n",
        "        self.n_rots = args.n_rots\n",
        "        self.d_out = args.d_out\n",
        "        self.eps = args.eps\n",
        "\n",
        "        self.n_epoch = args.n_epoch\n",
        "        if args.dataset == \"thyroid\" or args.dataset == \"arrhythmia\":\n",
        "            self.netC = netC1(self.d_out, self.ndf, self.n_rots).cuda()\n",
        "        else:\n",
        "            self.netC = netC5(self.d_out, self.ndf, self.n_rots).cuda()\n",
        "        weights_init(self.netC)\n",
        "        self.optimizerC = optim.Adam(self.netC.parameters(), lr=args.lr, betas=(0.5, 0.999))\n",
        "\n",
        "\n",
        "    def fit_trans_classifier(self, train_xs, x_test, y_test, ratio):\n",
        "        labels = torch.arange(self.n_rots).unsqueeze(0).expand((self.batch_size, self.n_rots)).long().cuda()\n",
        "        celoss = nn.CrossEntropyLoss()\n",
        "        print('Training')\n",
        "        for epoch in range(self.n_epoch):\n",
        "            self.netC.train()\n",
        "            rp = np.random.permutation(len(train_xs))\n",
        "            n_batch = 0\n",
        "            sum_zs = torch.zeros((self.ndf, self.n_rots)).cuda()\n",
        "\n",
        "            for i in range(0, len(train_xs), self.batch_size):\n",
        "                self.netC.zero_grad()\n",
        "                batch_range = min(self.batch_size, len(train_xs) - i)\n",
        "                train_labels = labels\n",
        "                if batch_range == len(train_xs) - i:\n",
        "                    train_labels = torch.arange(self.n_rots).unsqueeze(0).expand((len(train_xs) - i, self.n_rots)).long().cuda()\n",
        "                idx = np.arange(batch_range) + i\n",
        "                xs = torch.from_numpy(train_xs[rp[idx]]).float().cuda()\n",
        "                tc_zs, ce_zs = self.netC(xs)\n",
        "                sum_zs = sum_zs + tc_zs.mean(0)\n",
        "                tc_zs = tc_zs.permute(0, 2, 1)\n",
        "\n",
        "                loss_ce = celoss(ce_zs, train_labels)\n",
        "                er = self.lmbda * tc_loss(tc_zs, self.m) + loss_ce\n",
        "                er.backward()\n",
        "                self.optimizerC.step()\n",
        "                n_batch += 1\n",
        "\n",
        "            means = sum_zs.t() / n_batch\n",
        "            means = means.unsqueeze(0)\n",
        "            self.netC.eval()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                val_probs_rots = np.zeros((len(y_test), self.n_rots))\n",
        "                for i in range(0, len(x_test), self.batch_size):\n",
        "                    batch_range = min(self.batch_size, len(x_test) - i)\n",
        "                    idx = np.arange(batch_range) + i\n",
        "                    xs = torch.from_numpy(x_test[idx]).float().cuda()\n",
        "                    zs, fs = self.netC(xs)\n",
        "                    zs = zs.permute(0, 2, 1)\n",
        "                    diffs = ((zs.unsqueeze(2) - means) ** 2).sum(-1)\n",
        "\n",
        "                    diffs_eps = self.eps * torch.ones_like(diffs)\n",
        "                    diffs = torch.max(diffs, diffs_eps)\n",
        "                    logp_sz = torch.nn.functional.log_softmax(-diffs, dim=2)\n",
        "\n",
        "                    val_probs_rots[idx] = -torch.diagonal(logp_sz, 0, 1, 2).cpu().data.numpy()\n",
        "\n",
        "                val_probs_rots = val_probs_rots.sum(1)\n",
        "                f1_score = f_score(val_probs_rots, y_test, ratio)\n",
        "                print(\"Epoch:\", epoch, \", fscore: \", f1_score)\n",
        "        return f1_score\n"
      ],
      "id": "growing-incidence",
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inclusive-accident"
      },
      "source": [
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if isinstance(m, nn.Linear):\n",
        "        init.xavier_normal_(m.weight, gain=np.sqrt(2.0))\n",
        "    elif classname.find('Conv') != -1:\n",
        "        init.xavier_normal_(m.weight, gain=np.sqrt(2.0))\n",
        "    elif classname.find('Linear') != -1:\n",
        "        init.eye_(m.weight)\n",
        "    elif classname.find('Emb') != -1:\n",
        "        init.normal(m.weight, mean=0, std=0.01)\n",
        "\n",
        "class netC5(nn.Module):\n",
        "    def __init__(self, d, ndf, nc):\n",
        "        super(netC5, self).__init__()\n",
        "        self.trunk = nn.Sequential(\n",
        "        nn.Conv1d(d, ndf, kernel_size=1, bias=False),\n",
        "        nn.LeakyReLU(0.2, inplace=True),\n",
        "        nn.Conv1d(ndf, ndf, kernel_size=1, bias=False),\n",
        "        nn.LeakyReLU(0.2, inplace=True),\n",
        "        nn.Conv1d(ndf, ndf, kernel_size=1, bias=False),\n",
        "        nn.LeakyReLU(0.2, inplace=True),\n",
        "        nn.Conv1d(ndf, ndf, kernel_size=1, bias=False),\n",
        "        nn.LeakyReLU(0.2, inplace=True),\n",
        "        nn.Conv1d(ndf, ndf, kernel_size=1, bias=False),\n",
        "        )\n",
        "        self.head = nn.Sequential(\n",
        "        nn.LeakyReLU(0.2, inplace=True),\n",
        "        nn.Conv1d(ndf, nc, kernel_size=1, bias=True),\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, input):\n",
        "        tc = self.trunk(input)\n",
        "        ce = self.head(tc)\n",
        "        return tc, ce\n",
        "\n",
        "\n",
        "class netC1(nn.Module):\n",
        "    def __init__(self, d, ndf, nc):\n",
        "        super(netC1, self).__init__()\n",
        "        self.trunk = nn.Sequential(\n",
        "        nn.Conv1d(d, ndf, kernel_size=1, bias=False),\n",
        "        )\n",
        "        self.head = nn.Sequential(\n",
        "        nn.LeakyReLU(0.2, inplace=True),\n",
        "        nn.Conv1d(ndf, nc, kernel_size=1, bias=True),\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        tc = self.trunk(input)\n",
        "        ce = self.head(tc)\n",
        "        return tc, ce"
      ],
      "id": "inclusive-accident",
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h49bDQxZJZfa",
        "outputId": "03266b10-c155-4a3e-e967-5faac1c62529"
      },
      "source": [
        "np.random.randn(args.n_rots, 6, args.d_out).shape"
      ],
      "id": "h49bDQxZJZfa",
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(256, 6, 32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "southern-james"
      },
      "source": [
        "def load_trans_data(args):\n",
        "    dl = Data_Loader()\n",
        "    train_real, val_real, val_fake = dl.get_dataset(args.dataset, args.c_pr)\n",
        "    y_test_fscore = np.concatenate([np.zeros(len(val_real)), np.ones(len(val_fake))])\n",
        "    ratio = 100.0 * len(val_real) / (len(val_real) + len(val_fake))\n",
        "\n",
        "    n_train, n_dims = train_real.shape\n",
        "    rots = np.random.randn(args.n_rots, n_dims, args.d_out)\n",
        "\n",
        "    print('data trafo', train_real.shape, n_dims, rots[0].shape)\n",
        "    print('Calculating transforms')\n",
        "    x_train = np.stack([train_real.dot(rot) for rot in rots], 2)\n",
        "    val_real_xs = np.stack([val_real.dot(rot) for rot in rots], 2)\n",
        "    val_fake_xs = np.stack([val_fake.dot(rot) for rot in rots], 2)\n",
        "    x_test = np.concatenate([val_real_xs, val_fake_xs])\n",
        "    return x_train, x_test, y_test_fscore, ratio, rots\n",
        "\n",
        "\n",
        "def train_anomaly_detector(args):\n",
        "    x_train, x_test, y_test, ratio, _ = load_trans_data(args)\n",
        "    tc_obj = TransClassifierTabular(args)\n",
        "    f_score = tc_obj.fit_trans_classifier(x_train, x_test, y_test, ratio)\n",
        "    return f_score"
      ],
      "id": "southern-james",
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "suffering-connecticut",
        "outputId": "6f57006e-7f6d-438a-ad67-6af0ad7d5b50"
      },
      "source": [
        "args_dict = {'lr': 0.001,\n",
        "    'n_rots': 32,\n",
        "    'batch_size': 64, \n",
        "    'n_epoch': 25,\n",
        "    'd_out': 4,\n",
        "    'dataset': 'thyroid',\n",
        "    'exp': 'affine',\n",
        "    'c_pr': 0,\n",
        "    'true_label': 1, \n",
        "    'ndf': 8,\n",
        "    'm': 1,\n",
        "    'lmbda': 0.1,\n",
        "    'eps': 0,\n",
        "    'n_iters': 500}\n",
        "\n",
        "\n",
        "# python train_ad_tabular.py --n_rots=256 --n_epoch=1 --d_out=32 --ndf=8 --dataset=thyroid\n",
        "args = SimpleNamespace(**args_dict)\n",
        "args.n_rots = 256\n",
        "args.n_epoch = 1\n",
        "args.d_out = 32\n",
        "args.ndf = 8\n",
        "args.n_iters = 2\n",
        "\n",
        "print(\"Dataset: \", args.dataset)\n",
        "\n",
        "if args.dataset == 'thyroid' or args.dataset == 'arrhythmia':\n",
        "    n_iters = args.n_iters\n",
        "    f_scores = np.zeros(n_iters)\n",
        "    for i in range(n_iters):\n",
        "        f_scores[i] = train_anomaly_detector(args)\n",
        "    print(\"AVG f1_score\", f_scores.mean())\n",
        "else:\n",
        "    train_anomaly_detector(args)"
      ],
      "id": "suffering-connecticut",
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset:  thyroid\n",
            "data trafo (1839, 6) 6 (6, 32)\n",
            "Calculating transforms\n",
            "Training\n",
            "Epoch: 0 , fscore:  0.7419354838709677\n",
            "data trafo (1839, 6) 6 (6, 32)\n",
            "Calculating transforms\n",
            "Training\n",
            "Epoch: 0 , fscore:  0.7311827956989246\n",
            "AVG f1_score 0.7365591397849462\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RSAKFEYwHIkS",
        "outputId": "b9f933ed-ab13-4028-bbeb-9599adf416c5"
      },
      "source": [
        "x_train, x_test, y_test, ratio, rots = load_trans_data(args)\n",
        "tc_obj = TransClassifierTabular(args)\n",
        "f_score = tc_obj.fit_trans_classifier(x_train, x_test, y_test, ratio)"
      ],
      "id": "RSAKFEYwHIkS",
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data trafo (1839, 6) 6 (6, 32)\n",
            "Calculating transforms\n",
            "Training\n",
            "Epoch: 0 , fscore:  0.7419354838709677\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BdU9sfntIKad",
        "outputId": "77dfea01-a4fb-4ba0-e829-205701294f54"
      },
      "source": [
        "x_train.shape"
      ],
      "id": "BdU9sfntIKad",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1839, 32, 256)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYbOQGsBH6xH",
        "outputId": "264e5cad-e9be-4995-8975-36660c1beedc"
      },
      "source": [
        "tc_obj.netC"
      ],
      "id": "vYbOQGsBH6xH",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "netC1(\n",
              "  (trunk): Sequential(\n",
              "    (0): Conv1d(32, 8, kernel_size=(1,), stride=(1,), bias=False)\n",
              "  )\n",
              "  (head): Sequential(\n",
              "    (0): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "    (1): Conv1d(8, 256, kernel_size=(1,), stride=(1,))\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZiaChpAkHQ-",
        "outputId": "41950bec-52b7-4b0d-b6ba-4451f0420070"
      },
      "source": [
        "rots.shape"
      ],
      "id": "-ZiaChpAkHQ-",
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(256, 6, 32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4k2eiaS_gQ4s",
        "outputId": "91ff2e4c-e7bb-4aee-e385-a7715884eef2"
      },
      "source": [
        "tc_obj.netC.trunk[0].weight.shape"
      ],
      "id": "4k2eiaS_gQ4s",
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8, 32, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "520BE9cokPSV"
      },
      "source": [
        "data_rot = np.stack([np.array([[0,0,0,0,0,1]]).dot(rot) for rot in rots], 2)"
      ],
      "id": "520BE9cokPSV",
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sICyuq0Bcbr7",
        "outputId": "816eac81-40eb-43ca-ce01-c41b3fd9aaf7"
      },
      "source": [
        "data = torch.tensor(data_rot, dtype=torch.float)\n",
        "\n",
        "tc_obj.netC.trunk(data.to('cuda'))#.shape"
      ],
      "id": "sICyuq0Bcbr7",
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.5375, -0.1371,  0.6404,  ..., -0.2383, -0.6767, -0.3132],\n",
              "         [-0.3886, -1.3001, -0.3662,  ...,  0.7672,  0.3513,  0.2654],\n",
              "         [-0.1008, -0.4975, -0.9123,  ...,  0.7854, -0.3531, -0.8367],\n",
              "         ...,\n",
              "         [-0.0561,  0.6632,  0.1491,  ..., -0.8441, -0.7028,  0.9083],\n",
              "         [ 0.1384,  0.1224, -0.7818,  ..., -0.4075, -0.5954, -0.1950],\n",
              "         [ 0.8121, -0.1285, -0.3154,  ..., -0.3568, -0.0724,  0.1526]]],\n",
              "       device='cuda:0', grad_fn=<SqueezeBackward1>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5eghem2ybUN",
        "outputId": "2ff915c3-9026-415a-f65a-36356ba6572c"
      },
      "source": [
        "t1 = tc_obj.netC.trunk(data.to('cuda'))\n",
        "tc_obj.netC.head(t1).shape"
      ],
      "id": "g5eghem2ybUN",
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 256, 256])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oj3WkcyVHRNn",
        "outputId": "de7b4209-aaed-440e-ce7e-d98f4c94bbec"
      },
      "source": [
        "tc_obj.netC.trunk[0].weight.shape"
      ],
      "id": "Oj3WkcyVHRNn",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8, 32, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "driven-nepal"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--lr', default=0.001, type=float)\n",
        "    parser.add_argument('--n_rots', default=32, type=int)\n",
        "    parser.add_argument('--batch_size', default=64, type=int)\n",
        "    parser.add_argument('--n_epoch', default=25, type=int)\n",
        "    parser.add_argument('--d_out', default=4, type=int)\n",
        "    parser.add_argument('--dataset', default='thyroid', type=str)\n",
        "    parser.add_argument('--exp', default='affine', type=str)\n",
        "    parser.add_argument('--c_pr', default=0, type=int)\n",
        "    parser.add_argument('--true_label', default=1, type=int)\n",
        "    parser.add_argument('--ndf', default=8, type=int)\n",
        "    parser.add_argument('--m', default=1, type=float)\n",
        "    parser.add_argument('--lmbda', default=0.1, type=float)\n",
        "    parser.add_argument('--eps', default=0, type=float)\n",
        "    parser.add_argument('--n_iters', default=500, type=int)\n",
        "\n",
        "    args = parser.parse_args()\n",
        "    print(\"Dataset: \", args.dataset)\n",
        "\n",
        "    if args.dataset == 'thyroid' or args.dataset == 'arrhythmia':\n",
        "        n_iters = args.n_iters\n",
        "        f_scores = np.zeros(n_iters)\n",
        "        for i in range(n_iters):\n",
        "            f_scores[i] = train_anomaly_detector(args)\n",
        "        print(\"AVG f1_score\", f_scores.mean())\n",
        "    else:\n",
        "        train_anomaly_detector(args)\n"
      ],
      "id": "driven-nepal",
      "execution_count": null,
      "outputs": []
    }
  ]
}